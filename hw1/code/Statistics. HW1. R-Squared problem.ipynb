{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b43fef7",
   "metadata": {},
   "source": [
    "Author: Changalidi Anton \n",
    "\n",
    "27.02.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d1d45",
   "metadata": {},
   "source": [
    "# Part 1. Problem statement\n",
    "\n",
    "$R_2$ has a problem with datasets that have a large number of predictors with respect to the number of observations. Let's investigate it here!\n",
    "\n",
    "## Methods\n",
    "\n",
    "Several experiments were made using Monte-Carlo method: each parameter set was launched for 500 iterations, therefore the empirical (experimental) estimation will be close to the real theoretical value.\n",
    "\n",
    "Let: \n",
    "\n",
    "* $Y$ - target variable, amount of observations: $n_{obs}$; $dim(Y)$=$(n_{obs},1)$;\n",
    "* $X$ - all variables on which $Y$ actually depends, distribution: $N(0,1)$, it's amount: $n_{true}$;  $dim(X)$=$(n_{obs},n_{true})$;\n",
    "* $Z$ - noise variables (variables on which $Y$ does not depend), distribution: $N(0,1)$, it's amount: $n_{noise}$; $dim(Z)$=$(n_{obs},n_{noise})$.\n",
    "* $\\epsilon$ - error, $N(0,1)$.\n",
    "\n",
    "The real dependency will be: $Y = \\sum_{i=1}^{n_{true}}X_{i}+\\epsilon = \\sum_{i=1}^{n_{true}}1\\cdot X_{i}+ \\sum_{i=1}^{n_{noise}}0 \\cdot Z_{i}+\\epsilon$ (all $\\beta_i$ near $X$ are equal to $1$, and near $Z$ (noise variables) are equal to $0$).\n",
    "\n",
    "The prediction model will be: $Y = \\sum_{i=1}^{n_{true}}\\beta_{i} \\cdot X_{i} + \\sum_{i=1}^{n_{noise}}\\beta_{n_{true}+i} \\cdot Z_{i}+\\epsilon$. (**\\***)\n",
    "\n",
    "The experiments will be:\n",
    "\n",
    "* __$R^2$ ~ number of noise variables__: $n_{obs}=500$, $n_{true} \\in [1, 10]$, $n_{noise} \\in [0,1,10,50,100,200,500]$. In some experiments amount of noise variables $10$-$100$ times more, than amount of real predictors, that will perfectly depict $R^2$ distribution in such a situations.\n",
    "* __$R^2$ ~ number of noise variables and amount of observations__: $n_{obs} \\in [10, 50, 100, 500, 1000, 5000]$, $n_{true} = 1$, $n_{noise} \\in [0,1,10,50,100,200,500]$. Here true variable (just one) is fixed, and there will be  a lot of noised variables. The distribution of $r^2$ depending on amount of observations will be measured, leading to understanding the importance of the proper amount of samples.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c3f15b",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "For the first experiment (Fig. 1A and 1B) amount of noise variables increases the $R^2$: for the first experiment (only one true-influence variable): from $0.5$ ($0$ noise variables) to $0.75$ for ($500$ noise variables), which is a significant increase (variation is insignificant compared to the growth rate)! Even though $R^2$ was already high in the second experiment (due to the large number of true variables), the increase in $R^2$ with increasing noisy variables unrelated to the target is still clearly highlighted.\n",
    "\n",
    "The second experiment is perfectly depicted in Fig. 1D: $R^2$ grows as the ratio of the number of samples to the number of noisy variables increases, and grows till $\\frac{n_{noise}}{n_{obs}}= 10^0 = 1$, which is exactly when there are fewer data than variables in the analysis. Fig. 1C shows the same in a different way: the growth of $R^2$ is from $0.5$ to $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759d1d9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "![](figures/f1.png)\n",
    "\n",
    "**Figure 1**. A and B: Distribution of dependence of $R^2$ on the number of noise variables ($n_{true} \\in [1, 10]$ for A and B respectively). C and D: Distribution of dependence of $R^2$ on the number of noise variables and amount of observations (C: mean values are represented, D: every dot is an observation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8a1b61",
   "metadata": {},
   "source": [
    "# Part 2. Mathematical proof\n",
    "\n",
    "Note: In the real situations we do not know which variables are noise, and which have true influence on the target. That is why despite the fact, that here we operate with $x$ and $z$, $n_{true}$ and $n_{obs}$, in reality we will just have $x$ and $n$ (some of variables will be \"true\", some \"noise\"). But we separate it in the proof without loss of generality. \n",
    "\n",
    "Let $f_j$ - model's prediction, $\\overline{y}$ - average over all input $y_j$, then by definition (first equality) and from the (**\\***) of part one (second equality):\n",
    "$$R^2 = 1-\n",
    "\\frac{\n",
    "\\sum_{j=1}^{n_{obs}}(y_j-f_j)^2\n",
    "}{\n",
    "\\sum_{j=1}^{n_{obs}}(y_j-\\overline{y})^2\n",
    "} = 1-\n",
    "\\frac{\n",
    "\\sum_{j=1}^{n_{obs}}\n",
    "(\n",
    "y_j-(\n",
    "\\sum_{i=1}^{n_{true}}\\beta_{i} \\cdot x_{j,i} \n",
    "+ \n",
    "\\sum_{i=1}^{n_{noise}}\\beta_{n_{true}+i} \\cdot z_{j,i}\n",
    ")\n",
    ")^2\n",
    "}{\n",
    "\\sum_{j=1}^{n_{obs}}(y_j-\\overline{y})^2\n",
    "} \\text{   }$$  \n",
    "\n",
    "Denominator ($\\sum_{j=1}^{n_{obs}}(y_j-\\overline{y})^2$) does not depend on the model (so it is constant if dataset does not change).  In numerator we subtract from $y_j$: $\\sum_{i=1}^{n_{true}}\\beta_{i} \\cdot x_{j,i}$  and $\\sum_{i=1}^{n_{noise}}\\beta_{n_{true}+i} \\cdot z_{j,i}$. Here is where the problem comes from! \n",
    "\n",
    "If we will not take noise variables ($z_j$) into account, we will only subtract $\\sum_{i=1}^{n_{true}}\\beta_{i} \\cdot x_{j,i}$; however if we will add noised variables to the model. The best case scenario, if all $\\beta_i$ for $i \\in [n_{true}+1, n_{true}+n_{obs}]$ (coefficients before noised variables) are equal to zero: then we got the same model.\n",
    "\n",
    "\n",
    "Otherwise, we will also subtract $\\sum_{i=1}^{n_{noise}}\\beta_{n_{true}+i} \\cdot z_{j,i}$, making $\\sum_{j=1}^{n_{obs}}\n",
    "(\n",
    "y_j-(\n",
    "\\sum_{i=1}^{n_{true}}\\beta_{i} \\cdot x_{j,i} \n",
    "+ \n",
    "\\sum_{i=1}^{n_{noise}}\\beta_{n_{true}+i} \\cdot z_{j,i}\n",
    ")\n",
    ")^2$ smaller, \n",
    "therefore making $\\frac{\n",
    "\\sum_{j=1}^{n_{obs}}\n",
    "(\n",
    "y_j-(\n",
    "\\sum_{i=1}^{n_{true}}\\beta_{i} \\cdot x_{j,i} \n",
    "+ \n",
    "\\sum_{i=1}^{n_{noise}}\\beta_{n_{true}+i} \\cdot z_{j,i}\n",
    ")\n",
    ")^2\n",
    "}{\n",
    "\\sum_{j=1}^{n_{obs}}(y_j-\\overline{y})^2\n",
    "} \\text{   }$ smaller, \n",
    "and therefore making $R^2 = 1-\n",
    "\\frac{\n",
    "\\sum_{j=1}^{n_{obs}}\n",
    "(\n",
    "y_j-(\n",
    "\\sum_{i=1}^{n_{true}}\\beta_{i} \\cdot x_{j,i} \n",
    "+ \n",
    "\\sum_{i=1}^{n_{noise}}\\beta_{n_{true}+i} \\cdot z_{j,i}\n",
    ")\n",
    ")^2\n",
    "}{\n",
    "\\sum_{j=1}^{n_{obs}}(y_j-\\overline{y})^2\n",
    "} \\text{   }$ bigger with increasing number of noised variables.\n",
    "\n",
    "Q.E.D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36adea29",
   "metadata": {},
   "source": [
    "# Part 3. Solution\n",
    "\n",
    "The solution is to use adjusted $R^2$ (proposed by Mordecai Ezekiel, 1930).\n",
    "\n",
    "If amount of vaiables is $m (=n_{true}+n_{noise})$, then let $df_{model} = n_{obs} - m - 1$ and $df_{total} = n_{obs}-1$ (degrees of freedom, in model it is bigger, because we introduce variables), then:\n",
    "\n",
    "$$R^2_{adj} = 1-\\frac{\n",
    " \\frac{1}{df_{model}} \\cdot \\sum_{j=1}^{n_{obs}}(y_j-f_j)^2 \n",
    "}{\n",
    " \\frac{1}{df_{total}} \\cdot \\sum_{j=1}^{n_{obs}}(y_j-\\overline{y})^2\n",
    "} = 1-(1-R^2)\\frac{df_{total}}{df_{model}} = 1-(1-R^2)\\frac{n_{obs}-1}{n_{obs} - m - 1} $$\n",
    "\n",
    "\n",
    "Since $\\frac{df_{total}}{df_{model}}$ is more than one, we subtract bigger value from 1, making $R_{adj}^2 \\leq R^2$. The function penalises for each variable introduced. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd8677",
   "metadata": {},
   "source": [
    "# Part 4. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503367b7",
   "metadata": {},
   "source": [
    "We will take the Kaggle dataset for Real estate price prediction (Kaggle, 2018). It has 6 predictors, one target, amount of samples is 414. We will do linear regression on the original dataset, and then start to add $n_{noise} \\in [0, 1, 5, 10, 50, 100, 200]$ noise variables (from standard normal distribution), and build linear regression on this data. For each $n_{noise}$ we will do 500 iterations, and then observe distribution of $R^2$ and adjusted $R^2$.\n",
    "\n",
    "\n",
    "\n",
    "![](figures/f2.png)\n",
    "\n",
    "**Figure 2**. Distribution of $R^2$ and $R_{adj}^2$ as a function of noise variables amount.\n",
    "\n",
    "\n",
    "The results are presented on the Fig.2. The increase in $R^2$ with increasing number of noise variables is clearly visible (from $0.57$ to $0.78$), with $R_{adj}^2$ remaining almost constant (around $0.57$). \n",
    "\n",
    "That is why, in real experiments, researcher should be cautious if among many variables some variable gives an increase in $R^2$ (or other objective function, or it is significant in some statistical test). It is very very possible that it is just a random thing!  \n",
    "\n",
    "# Code availability\n",
    "\n",
    "All code pertinent to the results presented in this work are available at https://github.com/TohaRhymes/stat_um_24spring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbbf01",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Mordecai Ezekiel (1930), Methods Of Correlation Analysis, Wiley, pp. 208-211.\n",
    "\n",
    "[2] ALGOR_BRUCE. (2018). Real Estate Price Prediction [Dataset]. Kaggle. https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction?resource=download"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
